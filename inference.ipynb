{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fish Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Windows User / win用户"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "!chcp 65001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Linux User / Linux 用户"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n",
      "Fetching 7 files:   0%|                                   | 0/7 [00:00<?, ?it/s]Still waiting to acquire lock on checkpoints/fish-speech-1.5/.cache/huggingface/download/firefly-gan-vq-fsq-8x1024-21hz-generator.pth.lock (elapsed: 10.0 seconds)\n",
      "Still waiting to acquire lock on checkpoints/fish-speech-1.5/.cache/huggingface/download/model.pth.lock (elapsed: 10.0 seconds)\n",
      "Fetching 7 files:  43%|███████████▌               | 3/7 [00:11<00:15,  3.92s/it]\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ca/.local/lib/python3.8/site-packages/tqdm/contrib/concurrent.py\", line 51, in _executor_map\n",
      "    return list(tqdm_class(ex.map(fn, *iterables, chunksize=chunksize), **kwargs))\n",
      "  File \"/home/ca/.local/lib/python3.8/site-packages/tqdm/std.py\", line 1181, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/_base.py\", line 619, in result_iterator\n",
      "    yield fs.pop().result()\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/_base.py\", line 439, in result\n",
      "    self._condition.wait(timeout)\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 302, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ca/.local/bin/huggingface-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ca/.local/lib/python3.8/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 61, in main\n",
      "    service.run()\n",
      "  File \"/home/ca/.local/lib/python3.8/site-packages/huggingface_hub/commands/download.py\", line 157, in run\n",
      "    print(self._download())  # Print path to downloaded files\n",
      "  File \"/home/ca/.local/lib/python3.8/site-packages/huggingface_hub/commands/download.py\", line 191, in _download\n",
      "    return snapshot_download(\n",
      "  File \"/home/ca/.local/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ca/.local/lib/python3.8/site-packages/huggingface_hub/_snapshot_download.py\", line 332, in snapshot_download\n",
      "    thread_map(\n",
      "  File \"/home/ca/.local/lib/python3.8/site-packages/tqdm/contrib/concurrent.py\", line 69, in thread_map\n",
      "    return _executor_map(ThreadPoolExecutor, fn, *iterables, **tqdm_kwargs)\n",
      "  File \"/home/ca/.local/lib/python3.8/site-packages/tqdm/contrib/concurrent.py\", line 51, in _executor_map\n",
      "    return list(tqdm_class(ex.map(fn, *iterables, chunksize=chunksize), **kwargs))\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/_base.py\", line 644, in __exit__\n",
      "    self.shutdown(wait=True)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/thread.py\", line 236, in shutdown\n",
      "    t.join()\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# For Chinese users, you probably want to use mirror to accelerate downloading\n",
    "# !set HF_ENDPOINT=https://hf-mirror.com\n",
    "# !export HF_ENDPOINT=https://hf-mirror.com \n",
    "\n",
    "!huggingface-cli download fishaudio/fish-speech-1.5 --local-dir checkpoints/fish-speech-1.5/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WebUI Inference\n",
    "\n",
    "> You can use --compile to fuse CUDA kernels for faster inference (10x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python tools/run_webui.py \\\n",
    "    --llama-checkpoint-path checkpoints/fish-speech-1.5 \\\n",
    "    --decoder-checkpoint-path checkpoints/fish-speech-1.5/firefly-gan-vq-fsq-8x1024-21hz-generator.pth \\\n",
    "    # --compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break-down CLI Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Encode reference audio: / 从语音生成 prompt: \n",
    "\n",
    "You should get a `fake.npy` file.\n",
    "\n",
    "你应该能得到一个 `fake.npy` 文件."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "## Enter the path to the audio file here\n",
    "src_audio = r\"D:\\PythonProject\\vo_hutao_draw_appear.wav\"\n",
    "\n",
    "!python fish_speech/models/vqgan/inference.py \\\n",
    "    -i {src_audio} \\\n",
    "    --checkpoint-path \"checkpoints/fish-speech-1.5/firefly-gan-vq-fsq-8x1024-21hz-generator.pth\"\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "audio = Audio(filename=\"fake.wav\")\n",
    "display(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate semantic tokens from text: / 从文本生成语义 token:\n",
    "\n",
    "> This command will create a codes_N file in the working directory, where N is an integer starting from 0.\n",
    "\n",
    "> You may want to use `--compile` to fuse CUDA kernels for faster inference (~30 tokens/second -> ~300 tokens/second).\n",
    "\n",
    "> 该命令会在工作目录下创建 codes_N 文件, 其中 N 是从 0 开始的整数.\n",
    "\n",
    "> 您可以使用 `--compile` 来融合 cuda 内核以实现更快的推理 (~30 tokens/秒 -> ~300 tokens/秒)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python fish_speech/models/text2semantic/inference.py \\\n",
    "    --text \"hello world\" \\\n",
    "    --prompt-text \"The text corresponding to reference audio\" \\\n",
    "    --prompt-tokens \"fake.npy\" \\\n",
    "    --checkpoint-path \"checkpoints/fish-speech-1.5\" \\\n",
    "    --num-samples 2\n",
    "    # --compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate speech from semantic tokens: / 从语义 token 生成人声:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python fish_speech/models/vqgan/inference.py \\\n",
    "    -i \"codes_0.npy\" \\\n",
    "    --checkpoint-path \"checkpoints/fish-speech-1.5/firefly-gan-vq-fsq-8x1024-21hz-generator.pth\"\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "audio = Audio(filename=\"fake.wav\")\n",
    "display(audio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
